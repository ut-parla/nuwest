{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "# Tutorial 04: Data Movement\n",
    "\n",
    "This tutorial introduces how to move data between devices. \n",
    "- Data Movement - `clone_here` and `copy`, \n",
    "- Prefetched Data Movement - via Parla Managed Arrays (PArrays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "from time import perf_counter, sleep\n",
    "import os\n",
    "\n",
    "def set_numpy_threads(threads: int = 1):\n",
    "    \"\"\"\n",
    "    Numpy can be configured to use multiple threads for linear algebra operations.\n",
    "    The backend used by numpy can vary by installation.\n",
    "    This function attempts to set the number of threads for the most common backends.\n",
    "    MUST BE CALLED BEFORE IMPORTING NUMPY.\n",
    "\n",
    "    Args:\n",
    "        threads (int, optional): The number of threads to use. Defaults to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    os.environ[\"NUMEXPR_NUM_THREADS\"] = str(threads)\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(threads)\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = str(threads)\n",
    "    os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(threads)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(threads)\n",
    "\n",
    "    try:\n",
    "        # Controlling the MKL backend can use mkl and mkl-service modules if installed.\n",
    "        # preferred method for controlling the MKL backend.\n",
    "        import mkl\n",
    "\n",
    "        mkl.set_num_threads(threads)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "set_numpy_threads(1)\n",
    "import numpy as np \n",
    "import cupy as cp \n",
    "\n",
    "# Handle for Parla runtime\n",
    "from parla import Parla\n",
    "\n",
    "# Spawning task API\n",
    "from parla.tasks import (\n",
    "    spawn,\n",
    "    TaskSpace,\n",
    "    get_current_context,\n",
    "    get_current_task,\n",
    ")\n",
    "\n",
    "# Device Architectures for placement\n",
    "from parla.devices import cpu, gpu\n",
    "\n",
    "\n",
    "def run(function: Callable[[], Optional[TaskSpace]]):\n",
    "    assert callable(function), \"The function argument must be callable.\"\n",
    "\n",
    "    # Start the Parla runtime.\n",
    "    with Parla():\n",
    "        # Create a top-level task to kick off the computation\n",
    "        @spawn(placement=cpu, vcus=0)\n",
    "        async def top_level_task():\n",
    "            # Note that unless function returns a TaskSpace, this will NOT be blocking.\n",
    "            # If you want to wait on the completion of the tasks launched by function, you must return a TaskSpace that contains their terminal tasks.\n",
    "            await function()\n",
    "\n",
    "    # Runtime exists at the end of the context_manager\n",
    "    # All tasks are guaranteed to be complete at this point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Data Movement Annotations - `clone_here` and `copy`\n",
    "\n",
    "Parla provides two functions for moving data between devices with a task: `clone_here` and `copy`. \n",
    "- `clone_here` copies the data to the device where the task is running.\n",
    "- `copy` writes into a data buffer from any source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parla.array import clone_here, copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: `tutorial/14_manual_data_movement.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task(global_1)  running on  GPUEnvironment(GPU:0)\n",
      "C is a Cupy Array on GPU[<CUDA Device 0>]\n"
     ]
    }
   ],
   "source": [
    "def clone_here_copy_example_wrapper():\n",
    "    import numpy as np\n",
    "    import cupy as cp\n",
    "    M = 5\n",
    "    N = 5\n",
    "    A = np.random.rand(M)\n",
    "    B = cp.arange(N)\n",
    "    \n",
    "    def clone_here_copy_example():\n",
    "        T = TaskSpace(\"T\")\n",
    "        \n",
    "        @spawn(placement=[cpu if np.random.rand() < 0.5 else gpu])\n",
    "        def task():\n",
    "            print(get_current_task(), \" running on \", get_current_context())\n",
    "            C = clone_here(A)\n",
    "            print(\"C is a\", \"Numpy Array\" if isinstance(C, np.ndarray) else f\"Cupy Array on GPU[{C.device}]\", flush=True)\n",
    "            \n",
    "        return T\n",
    "    \n",
    "    run(clone_here_copy_example)\n",
    "        \n",
    "clone_here_copy_example_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefetched Data Movement - via Parla Managed Arrays (PArrays)\n",
    "\n",
    "Parla provides a data structure for data movement and coherence across devices: `PArrays`. \n",
    "PArrays are a lightweight wrapper around CuPy and NumPy ndarrays that:\n",
    "- Allow the runtime to track the location of the data\n",
    "- Allow the runtime to prefetch data to the device where a task will run\n",
    "- And main a coherence protcol for multiple valid distributed copies of the same data. \n",
    "\n",
    "PArrays can be created directly from NumPy or CuPy ndarrays. \n",
    "\n",
    "```python \n",
    "import numpy as np\n",
    "A = np.random.rand(1000, 1000)\n",
    "A_wrapped = parla.array.asarray(A)\n",
    "```\n",
    "\n",
    "Once an array has been wrapped, it can be used in the dataflow arguments of `@spawn`. \n",
    "The runtime will automatically prefetch the data to the device where the task will run. \n",
    "`input` moves the data to the device and makes it available for reading.\n",
    "`inout` moves the data to the device and makes it available for reading and writing. \n",
    "This invalidates any other copies of the data.\n",
    "\n",
    "```python\n",
    "@spawn(placement=gpu, inout=[A_wrapped])\n",
    "def compute_A(A):\n",
    "    # Do something with A on the GPU\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: `tutorial/15_prefetched_data_movement.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task(global_1)  running on  GPUEnvironment(GPU:0)\n",
      "A is a Cupy Array on GPU[<CUDA Device 0>]\n",
      "---Overview of PArray\n",
      "ID: 137737518180896, Name: NA, Parent_ID: None, Slice: None, Bytes: 40, Owner: GPU 0\n",
      "At GPU 0: state: SHARED\n",
      "At CPU: state: SHARED\n",
      "---End of Overview\n"
     ]
    }
   ],
   "source": [
    "from parla.array import asarray as parla_asarray\n",
    "\n",
    "\n",
    "async def parray_example():\n",
    "    A = np.random.rand(5)\n",
    "    A = parla_asarray(A)\n",
    "\n",
    "    @spawn(placement=[cpu if np.random.rand() < 0.5 else gpu], input=[A])\n",
    "    def task():\n",
    "        print(get_current_task(), \" running on \", get_current_context())\n",
    "        print(\n",
    "            \"A is a\",\n",
    "            \"Numpy Array\"\n",
    "            if isinstance(A.array, np.ndarray)\n",
    "            else f\"Cupy Array on GPU[{A.array.device}]\",\n",
    "            flush=True,\n",
    "        )\n",
    "        A.print_overview()\n",
    "        #There is a valid copy of A on both devices\n",
    "\n",
    "\n",
    "run(parray_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: `tutorial/16_write_invalidation.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task(T_0)  running on  GPUEnvironment(GPU:0)\n",
      "A is a Cupy Array on GPU[<CUDA Device 0>]\n",
      "---Overview of PArray\n",
      "ID: 137737518206128, Name: NA, Parent_ID: None, Slice: None, Bytes: 40, Owner: GPU 0\n",
      "At GPU 0: state: SHARED\n",
      "At CPU: state: SHARED\n",
      "---End of Overview\n",
      "[1. 1. 1. 1. 1.]\n",
      "\n",
      "\n",
      "Task(T_1)  running on  GPUEnvironment(GPU:0)\n",
      "A is a Cupy Array on GPU[<CUDA Device 0>]\n",
      "---Overview of PArray\n",
      "ID: 137737518206128, Name: NA, Parent_ID: None, Slice: None, Bytes: 40, Owner: GPU 0\n",
      "At GPU 0: state: MODIFIED\n",
      "At CPU: state: INVALID\n",
      "---End of Overview\n",
      "\n",
      "\n",
      "Task(T_2)  running on  GPUEnvironment(GPU:0)\n",
      "A is a Cupy Array on GPU[<CUDA Device 0>]\n",
      "---Overview of PArray\n",
      "ID: 137737518206128, Name: NA, Parent_ID: None, Slice: None, Bytes: 40, Owner: GPU 0\n",
      "At GPU 0: state: MODIFIED\n",
      "At CPU: state: INVALID\n",
      "---End of Overview\n",
      "[2. 2. 2. 2. 2.]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def parray_example():\n",
    "    T = TaskSpace(\"T\")\n",
    "\n",
    "    A = np.ones(5)\n",
    "    A = parla_asarray(A)\n",
    "\n",
    "    @spawn(T[0], placement=[cpu if np.random.rand() < 0.5 else gpu], input=[A])\n",
    "    def task():\n",
    "        print(get_current_task(), \" running on \", get_current_context())\n",
    "        print(\n",
    "            \"A is a\",\n",
    "            \"Numpy Array\"\n",
    "            if isinstance(A.array, np.ndarray)\n",
    "            else f\"Cupy Array on GPU[{A.array.device}]\",\n",
    "            flush=True,\n",
    "        )\n",
    "        A.print_overview()\n",
    "        print(A.array)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    @spawn(T[1], [T[0]], placement=[cpu if np.random.rand() < 0.5 else gpu], inout=[A])\n",
    "    def task():\n",
    "        print(get_current_task(), \" running on \", get_current_context())\n",
    "        print(\n",
    "            \"A is a\",\n",
    "            \"Numpy Array\"\n",
    "            if isinstance(A.array, np.ndarray)\n",
    "            else f\"Cupy Array on GPU[{A.array.device}]\",\n",
    "            flush=True,\n",
    "        )\n",
    "        A.print_overview()\n",
    "        A[:] = A + 1\n",
    "        print(\"\\n\")\n",
    "\n",
    "    @spawn(\n",
    "        T[2],\n",
    "        [T[0], T[1]],\n",
    "        placement=[cpu if np.random.rand() < 0.5 else gpu],\n",
    "        inout=[A],\n",
    "    )\n",
    "    def task():\n",
    "        print(get_current_task(), \" running on \", get_current_context())\n",
    "        print(\n",
    "            \"A is a\",\n",
    "            \"Numpy Array\"\n",
    "            if isinstance(A.array, np.ndarray)\n",
    "            else f\"Cupy Array on GPU[{A.array.device}]\",\n",
    "            flush=True,\n",
    "        )\n",
    "        A.print_overview()\n",
    "        print(A.array)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "run(parray_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
