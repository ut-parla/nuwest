{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "# Tutorial 01: Introduction to Parla\n",
    "\n",
    "This tutorial introduces basic concepts in task-based parallel programming using Parla.\n",
    "We will cover:\n",
    "- Installation\n",
    "- What is Parla?\n",
    "- Running Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Parla?\n",
    "\n",
    "Parla is a task-parallel programming library for Python. Parla targets the orchestration of heterogeneous (CPU+GPU) workloads on a single shared-memory machine. We provide features for resource management, task variants, and automated scheduling of data movement between devices.\n",
    "\n",
    "We design for gradual-adoption allowing users to easily port sequential code for parallel execution.\n",
    "\n",
    "The Parla runtime is multi-threaded but single-process to utilize a shared address space. In practice, this means that the main workload within each task *must* release CPython's Global Interpreter Lock (GIL) to allows tasks to gain parallel speedup. This includes many common numerical libraries such as NumPy, SciPy, Numba, and PyKokkos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "We provide a preconfigued Docker container for running Parla. \n",
    "This is the easiest way to get started with Parla, and is the recommended way to run the tutorial.\n",
    "Please refer to the [README](../../../README.md) for instructions using the provided container.\n",
    "\n",
    "Briefly, to install Parla on a local machine you will need to clone the repository and install it using pip.\n",
    "\n",
    "```bash\n",
    "python -m install numpy scipy\n",
    "git clone https://github.com/ut-parla/parla-experimental.git\n",
    "cd parla-experimental\n",
    "git submodule update --init --recursive\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Runnings Tasks\n",
    "Parallel programming in Python is centered around ***tasks***. \n",
    "Tasks are annotated blocks of Python code that may be executed asynchronously and concurrently with each other.\n",
    "\n",
    "Parla tasks can be annotated with various constraints and properties that allow the runtime to make intelligent scheduling decisions about where and when they should execute. We'll cover these in more detail in later tutorials.\n",
    "\n",
    "For the moment we'll look at the simplest possible tasks and how to run them.\n",
    "\n",
    "### Example 1: `tutorial/01_hello.py`\n",
    "\n",
    "First, we'll import the Parla runtime and some of its utilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "from time import perf_counter, sleep\n",
    "\n",
    "# Handle for Parla runtime\n",
    "from parla import Parla\n",
    "\n",
    "# Spawning  tasks\n",
    "from parla.tasks import spawn, TaskSpace \n",
    "from parla.devices import cpu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scope of the Parla runtime is defined by a `Parla()` context manager.\n",
    "This context manager is used to initialize the runtime and ensure that all tasks have completed before exiting the program.\n",
    "\n",
    "```python\n",
    "with Parla():\n",
    "    # Parla runtime is initialized here\n",
    "    # Tasks may be submitted and executed\n",
    "# Parla runtime is finalized here\n",
    "```\n",
    "\n",
    "Although, this can be done within the global namespace of a Python program, it is typically bad practice to do so.\n",
    "Within a task, global variables are not captured by value and may change before the task is executed.\n",
    "\n",
    "To avoid this, in this tutorial we provide a wrapper function `run` that will execute a program within a `Parla()` context manager. This helps ensure that tasks are declared locally and provides a top-level task to orchestrate the execution of the program.\n",
    "\n",
    "Don't worry about the details of this function for now, we'll cover them in more detail in later. For now, just know that you should wrap your program in a call to `run` to ensure that it executes correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(function: Callable[[], Optional[TaskSpace]]):\n",
    "    assert callable(function), \"The function argument must be callable.\"\n",
    "\n",
    "    # Start the Parla runtime.\n",
    "    with Parla():\n",
    "        # Create a top-level task to kick off the computation\n",
    "        @spawn(placement=cpu, vcus=0)\n",
    "        async def top_level_task():\n",
    "            await function()\n",
    "            \n",
    "    # Runtime exists at the end of the context_manager\n",
    "    # All tasks are guaranteed to be complete at this point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at spawning a task.\n",
    "\n",
    "In Parla, tasks are defined and launched using the `@spawn` decorator. \n",
    "The `@spawn` decorator captures the code block and submits it to the runtime for asynchronous execution.\n",
    "*As soon as the task is submitted, the runtime is free to schedule it.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running my_first_task example...\n",
      "Hello from the task!\n"
     ]
    }
   ],
   "source": [
    "# A simple task that prints a message\n",
    "async def my_first_task():\n",
    "    @spawn()\n",
    "    def hello():\n",
    "        print(\"Hello from the task!\")\n",
    "\n",
    "print(\"Running my_first_task example...\")\n",
    "run(my_first_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: `tutorial/02_independent_tasks.py`\n",
    "\n",
    "Tasks run concurrently with respect to each other and the main program.\n",
    "Below, we spawn 4 embarrassingly parallel tasks and print a message from each one. \n",
    "While the tasks are more likely to execute in the order they are spawned, the runtime is free to schedule them in any order. \n",
    "This means that the order of the printed messages may vary between runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running independent_tasks example...\n",
      "Hello from Task 0! \n",
      "\n",
      "Hello from Task 2! \n",
      "\n",
      "Hello from Task 1! \n",
      "\n",
      "Hello from Task 3! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def independent_tasks():\n",
    "    n_tasks = 4\n",
    "    for i in range(n_tasks):\n",
    "        @spawn()\n",
    "        def task():\n",
    "            # Local variables are captured by a shallow copy of everythig in the closure of `task()`\n",
    "            # i is captured by value and is not shared between tasks\n",
    "            print(f\"Hello from Task {i}! \\n\", flush=True)\n",
    "            # flush=True is needed to ensure that the print statement is not buffered.\n",
    "\n",
    "print(\"Running independent_tasks example...\")\n",
    "run(independent_tasks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables are passed into tasks by being captured in the annotated function's \"closure\".\n",
    "All variables in the local scope where a task is spawned are captured by *shallow copy*. \n",
    "\n",
    "Value types (like integer, double, string, tuple) are copied by value, but compound objects with internal reference types (like list, dictionary, numpy arrays, etc.) share their memory pointers between tasks.\n",
    "\n",
    "**Note that this is different from the default behavior of Python functions, which capture all variables by reference in the closure.**\n",
    "\n",
    "### Example 3: `tutorial/03_shared_memory.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running independent_tasks_dictionary example...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared Dictionary:  {0: 0, 1: 1, 2: 4, 3: 9}\n"
     ]
    }
   ],
   "source": [
    "async def independent_tasks_dictionary():\n",
    "    n_tasks = 4\n",
    "    shared_dict = {}\n",
    "    for i in range(n_tasks):\n",
    "        @spawn()\n",
    "        def task():\n",
    "            # Local variables are captured by a shallow copy of everything in the closure of `task()`\n",
    "            # Python primitives are thread safe (locking) \n",
    "            shared_dict[i] = i**2\n",
    "    \n",
    "    # For now, we need to sleep to ensure that the tasks have completed\n",
    "    # Later, we'll discuss barriers, returns, and general control flow\n",
    "    sleep(0.1)\n",
    "    print(\"Shared Dictionary: \", shared_dict)\n",
    "    \n",
    "print(\"Running independent_tasks_dictionary example...\")\n",
    "run(independent_tasks_dictionary)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing output through shared objects is the most common way to communicate between tasks in Parla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Advice for Writing Effective Parla Tasks \n",
    "\n",
    "Unlike many Python tasking systems, Parla tasks are run within a thread-based environment. All tasks execute within the same process and, unfortunately, share the same Python interpreter (if run with CPython). All tasks need to acquire the Python Global Interpreter Lock (GIL) to execute any lines of native Python code. This means any pure Python will execute serially and not show parallel speedup.\n",
    "\n",
    "Tasks only achieve true parallelism when they call out to compiled libraries and external code that releases the GIL, such as Numpy, Cupy, or jit-compiled Numba kernels. Parla is well-suited for parallelism in compute-heavy domains, but less-suited to workloads that need to execute many routines with native-Python-implemented libraries (like SymPy).\n",
    "\n",
    "To write code that performs well in Parla, tasks should avoid holding and accessing the GIL as much as possible. For a 50ms task, the GIL should be held for less than 5% of the total task-time to avoid noticeable overheads.\n",
    "\n",
    "Launching tasks with threads, however, does give us some advantages. Tasks share the same address space, allowing copyless operations on any memory buffers. We do not need to worry about managing or importing separate module lists in different persistent-Python processes, and any jit compilation by Numba or other external libraries will be automatically reused between subsequent tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
