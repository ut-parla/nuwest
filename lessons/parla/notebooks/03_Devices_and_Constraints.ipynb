{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "# Tutorial 03: Devices and Constraints\n",
    "\n",
    "So far all the examples have been using the default (CPU) only. \n",
    "\n",
    "In general Parla can manage tasks across heterogenous hardware (CPUs, GPUs)\n",
    "\n",
    "**Note: This tutorial requires at least 1 GPU available**\n",
    "\n",
    "Here we will cover:\n",
    "- Task Device Placement \n",
    "- Task Constraints (memory, compute resources)\n",
    "- Function Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_PYTHON_RUNAHEAD:  True\n",
      "CUPY_ENABLED:  True\n",
      "PREINIT_THREADS:  True\n",
      "DEFAULT SYNC:  0\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Optional\n",
    "from time import perf_counter, sleep\n",
    "import os\n",
    "\n",
    "def set_numpy_threads(threads: int = 1):\n",
    "    \"\"\"\n",
    "    Numpy can be configured to use multiple threads for linear algebra operations.\n",
    "    The backend used by numpy can vary by installation.\n",
    "    This function attempts to set the number of threads for the most common backends.\n",
    "    MUST BE CALLED BEFORE IMPORTING NUMPY.\n",
    "\n",
    "    Args:\n",
    "        threads (int, optional): The number of threads to use. Defaults to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    os.environ[\"NUMEXPR_NUM_THREADS\"] = str(threads)\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(threads)\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = str(threads)\n",
    "    os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(threads)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(threads)\n",
    "\n",
    "    try:\n",
    "        # Controlling the MKL backend can use mkl and mkl-service modules if installed.\n",
    "        # preferred method for controlling the MKL backend.\n",
    "        import mkl\n",
    "\n",
    "        mkl.set_num_threads(threads)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "set_numpy_threads(1)\n",
    "\n",
    "# Handle for Parla runtime\n",
    "from parla import Parla\n",
    "\n",
    "# Spawning task API\n",
    "from parla.tasks import (\n",
    "    spawn,\n",
    "    TaskSpace,\n",
    "    get_current_context,\n",
    "    get_current_task,\n",
    ")\n",
    "\n",
    "# Device Architectures for placement\n",
    "from parla.devices import cpu, gpu\n",
    "\n",
    "\n",
    "def run(function: Callable[[], Optional[TaskSpace]]):\n",
    "    assert callable(function), \"The function argument must be callable.\"\n",
    "\n",
    "    # Start the Parla runtime.\n",
    "    with Parla():\n",
    "        # Create a top-level task to kick off the computation\n",
    "        @spawn(placement=cpu, vcus=0)\n",
    "        async def top_level_task():\n",
    "            # Note that unless function returns a TaskSpace, this will NOT be blocking.\n",
    "            # If you want to wait on the completion of the tasks launched by function, you must return a TaskSpace that contains their terminal tasks.\n",
    "            await function()\n",
    "\n",
    "    # Runtime exists at the end of the context_manager\n",
    "    # All tasks are guaranteed to be complete at this point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Placement\n",
    "\n",
    "The placement argument to `@spawn()` determines where the task will be placed.\n",
    "\n",
    "It can be a single device type or a list of possible placement options. \n",
    "\n",
    "At runtime, Parla will choose the most available option using its mapping policy, typically considering available resources and load balancing.\n",
    "\n",
    "When running this tutorial, you may see different devices being used for the \"either\" task.\n",
    "\n",
    "(Note the runtime may have a preference for scheduling on your GPU)\n",
    "\n",
    "#### Example: `tutorial/11_task_placement.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from Task(Device_cpu_0), running on CPUEnvironment(CPU:0)\n",
      "Hello from Task(Device_gpu_0), running on GPUEnvironment(GPU:0)\n",
      "Hello from Task(Device_cpu_1), running on CPUEnvironment(CPU:0)\n",
      "Hello from Task(Device_either_0), running on GPUEnvironment(GPU:0)\n",
      "Hello from Task(Device_gpu_1), running on GPUEnvironment(GPU:0)\n",
      "Hello from Task(Device_either_1), running on GPUEnvironment(GPU:0)\n"
     ]
    }
   ],
   "source": [
    "async def device_task():\n",
    "    T = TaskSpace(\"Device\")\n",
    "\n",
    "    for i in range(2):\n",
    "        @spawn(T[\"cpu\", i], placement=cpu)\n",
    "        def cpu_task():\n",
    "            # Runs on a CPU device\n",
    "            print(f\"Hello from {get_current_task()}, running on {get_current_context()}\")\n",
    "\n",
    "        @spawn(T[\"gpu\", i], placement=gpu)\n",
    "        def gpu_task():\n",
    "            # Runs on a GPU device \n",
    "            print(f\"Hello from {get_current_task()}, running on {get_current_context()}\")\n",
    "            \n",
    "        @spawn(T[\"either\", i], placement=[cpu, gpu])\n",
    "        def either_task():\n",
    "            # Runs on either a CPU or GPU device\n",
    "            print(f\"Hello from {get_current_task()}, running on {get_current_context()}\")\n",
    "            \n",
    "        \n",
    "run(device_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the placement listed the device architecture type which allowed the runtime to schedule the task on any instance of that type of device. \n",
    "\n",
    "Specific devices can be specified by listing an index. For example, `placement=[gpu(i)]` means the task must be placed on the i-th GPU. \n",
    "\n",
    "### GPU Tasks \n",
    "\n",
    "GPU Tasks are still hosted on the CPU. The task is assigned to a host thread and it's python code block is executed. \n",
    "A GPU task does not need to be \"pure\" and can still execute CPU functions.\n",
    "\n",
    "Using a GPU task has the following benefits:\n",
    "- The active CuPy device context is set to the chosen GPU device \n",
    "- A CUDA/HIP Stream is pulled from the device's stream pool and set as the active stream\n",
    "- An event for the task's kernel completion is created on the device's stream\n",
    "\n",
    "## Task Constraints\n",
    "\n",
    "The `@spawn()` decorator can also be given constraints on the resources required by the task.\n",
    "There are two main resource types:\n",
    "- Memory, the size (bytes) of non-persistent workspace the task is expected to use during its lifetime\n",
    "- Virtual Compute Unites (VCUs), the expected fraction of the device the task will use \n",
    "\n",
    "They can be listed in spawn as `memory` and `vcus` respectively:\n",
    "`@spawn(placement=[gpu], memory=1e9, vcus=0.5)`\n",
    "\n",
    "### Virtual Compute Units\n",
    "\n",
    "VCUs limit the parallelism on each device. For example, `vcus=0.5` means the task will use half of the device's compute resources and that two tasks of this weight could be scheduled onto that device concurrently.\n",
    "\n",
    "Each task that runs concurrently on the same GPU gets its own CUDA stream.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: `tutorial/12_task_constraints.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: Task(T_0)\n",
      "Completed:  Task(T_0)\n",
      "Starting: Task(T_1)\n",
      "Completed:  Task(T_1)\n",
      "Starting: Task(T_2)\n",
      "Completed:  Task(T_2)\n",
      "Starting: Task(T_3)\n",
      "Completed:  Task(T_3)\n",
      "Starting: Task(T_4)\n",
      "Completed:  Task(T_4)\n",
      "Starting sum\n",
      "Finished sum\n",
      "Elapsed Time:  6.829967883008067\n"
     ]
    }
   ],
   "source": [
    "async def vcu_example():\n",
    "    \n",
    "    T = TaskSpace(\"T\")\n",
    "    import numpy as np\n",
    "    N = 10000\n",
    "    n_tasks = 5\n",
    "    \n",
    "    # Try changing the cost to increase parallelism\n",
    "    cost = 1 # Serial\n",
    "    # cost = 1/8 # 2 Active CPU Threads\n",
    "    # cost = 1/4 # 4 Active CPU Threads\n",
    "    \n",
    "    start_t = perf_counter()\n",
    "    vectors = [np.random.rand(N) for _ in range(n_tasks)]\n",
    "    matricies = [np.random.rand(N, N) for _ in range(n_tasks)]\n",
    "    \n",
    "    for i in range(n_tasks):\n",
    "        @spawn(T[i], placement=cpu, vcus=cost)\n",
    "        def task():\n",
    "            print(\"Starting:\", T[i], flush=True)\n",
    "            v = vectors[i]\n",
    "            M = matricies[i]\n",
    "            for _ in range(1):\n",
    "                v = M @ v\n",
    "            print(\"Completed: \", T[i], flush=True)\n",
    "    \n",
    "    @spawn(T[\"sum\"], [T[:n_tasks]], placement=cpu, vcus=cost)\n",
    "    def sum_task():\n",
    "        print(\"Starting sum\", flush=True)\n",
    "        vectors[0] = sum(vectors)\n",
    "        print(\"Finished sum\", flush=True)\n",
    "        \n",
    "    await T\n",
    "    end_t = perf_counter()\n",
    "    \n",
    "    print(\"Elapsed Time: \", end_t - start_t)\n",
    "    \n",
    "run(vcu_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Variants\n",
    "\n",
    "As tasks can be both CPU and GPU it can be advantageous to have different implementations of the same function for each device type. \n",
    "\n",
    "Parla provides annotations to overload functions and dispatch them based on the current task's device context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: `tutorial/13_function_variants.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parla.tasks import specialize\n",
    "from parla.array import clone_here\n",
    "import numpy as np\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@specialize\n",
    "def function(A: np.ndarray):\n",
    "    print(\"Running Default Implementation\", flush=True)\n",
    "    return np.linalg.eigh(A)\n",
    "    \n",
    "    \n",
    "@function.variant(gpu)\n",
    "def function_gpu(A: cp.ndarray):\n",
    "    print(\"Running GPU Implementation\", flush=True)\n",
    "    return cp.linalg.eigh(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CPU Task\n",
      "Running Default Implementation\n",
      "Completed CPU Task\n",
      "Running GPU Task\n",
      "Running GPU Implementation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed GPU Task\n"
     ]
    }
   ],
   "source": [
    "def specialization_example():\n",
    "    A = np.random.rand(1000, 1000)\n",
    "    B = np.copy(A)\n",
    "    T = TaskSpace(\"T\")\n",
    "    \n",
    "    @spawn(T[0], placement=cpu)\n",
    "    def t1():\n",
    "        print(\"Running CPU Task\", flush=True)\n",
    "        A_local = clone_here(A)\n",
    "        C = function(A_local)\n",
    "        print(\"Completed CPU Task\", flush=True)\n",
    "        \n",
    "    @spawn(T[1], [T[0]], placement=gpu)\n",
    "    def t2():\n",
    "        print(\"Running GPU Task\", flush=True)\n",
    "        B_local = clone_here(B)\n",
    "        C = function(B_local)\n",
    "        print(\"Completed GPU Task\", flush=True)\n",
    "        \n",
    "    return  T\n",
    "    \n",
    "run(specialization_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
